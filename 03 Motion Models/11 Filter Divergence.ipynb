{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef3642-a503-4ab2-8225-66cf818e49aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a1f98-70df-420d-94c0-227943e993d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef848e20-d866-402f-9772-ec7ee3efc081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import plotly.express as ex\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from copy import deepcopy\n",
    "import cache_magic\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "from tracking_v2.target import ConstantVelocityTarget, SingerTarget\n",
    "from tracking_v2.kalman import LinearKalmanFilter\n",
    "from tracking_v2.motion import ConstantVelocityModel, ConstantAccelerationModel, SingerAccelerationModel\n",
    "from tracking_v2.sensor import GeometricSensor\n",
    "from tracking_v2.evaluation import Runner, run_many, evaluate_many, plot_nees, evaluate_nees, \\\n",
    "                                   plot_error, evaluate_runner, plot_2d, plot_3d\n",
    "\n",
    "from tracking.util import to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e5c47-29b5-4ec7-aa58-3c01b8034fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def two_columns(fig1, fig2):\n",
    "    output1 = widgets.Output(layout={'width': '50%'})\n",
    "    with output1:\n",
    "        display(fig1)\n",
    "\n",
    "    output2 = widgets.Output(layout={'width': '50%'})\n",
    "    with output2:\n",
    "        display(fig2)\n",
    "    \n",
    "    column_layout = widgets.HBox([output1, output2])\n",
    "    display(column_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5a9e1-9e26-4a55-b3e5-98de8f7c4147",
   "metadata": {},
   "source": [
    "# Target and sensor\n",
    "\n",
    "In this document we will consider a simple target moving with constant velocity along the X axis. The sensor produces 3D measurement in the Cartesian space with unit covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0487b21-397d-4501-aa3a-131523cb70f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = ConstantVelocityTarget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91cb00-8dff-4d9b-9a19-dad0b182543f",
   "metadata": {},
   "source": [
    "# Kalman Filter drift\n",
    "\n",
    "In some situations, Kalman Filter's error has the tendency to drift - accumulate over time to large values of NEES.\n",
    "\n",
    "Our target is moving with a perfectly constant velocity. The appropriate motion noise model would be of no noise, $Q = 0$. However, this leads to state covariance $\\hat{P}$ converging to zero which places majority of \"trust\" into the current state estimate and increasingly less into each new measurement. With incorrect velocity estimates, this can push the state estimate to be quite off while the expected variance is minuscule.\n",
    "\n",
    "Let's compare two situations: a KF with zero process noise and another with CV motion model with noise set to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb2ed0-522d-44f1-8d5a-89cb17b6f051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _drift_one(q, target):\n",
    "    motion = ConstantVelocityModel(q)\n",
    "    kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                     [0, 1, 0, 0, 0, 0],\n",
    "                                     [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "    sensor = GeometricSensor(seed=8)\n",
    "    r = Runner(target, sensor, kf)\n",
    "    r.run_many(1, 500)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6de2aa-8f03-4ecb-a3c7-cbdafbd1fa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cache cv_q0_one = _drift_one(0, target)\n",
    "%cache cv_q1_one = _drift_one(1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9682d1-d383-433b-80e1-23209279ef21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _plot_error_one(runner):\n",
    "    tm  = np.arange(runner.n-100).reshape((runner.n-100, -1))\n",
    "    err = np.hstack((tm, np.abs(runner.one_x_hat[100:,:3,0] - runner.truth[101:,:3])))\n",
    "    err = to_df(err, columns=['time', 'x', 'y', 'z']).melt(['time'], ['x', 'y', 'z'], 'dim', 'error')\n",
    "    return ex.line(err, x='time', y='error', facet_row='dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4399b-51c1-4d4f-8ea1-ae6e7a58d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_columns(_plot_error_one(cv_q0_one), _plot_error_one(cv_q1_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b385b2-6ade-4290-9e01-b77b630aeb58",
   "metadata": {},
   "source": [
    "In the left-hand-side plot above, the absolute error accumulates up to about $0.5$ while in the right-hand-side plot the error does not show the pattern of accumulation, even though it consistently reaches much higher values, up to about $4$. However, the apparent advantage of the model with $Q=0$ disappears once we look at normalized errors (NEES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a8f40-c092-4936-ab62-b8a59e55177b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _plot_nees_one(runner):\n",
    "    nees = evaluate_nees(runner.one_x_hat[:, :3, :], runner.one_P_hat[:, :3, :3], runner.truth[1:, :3])\n",
    "    err = np.asarray((np.arange(runner.n-100), nees.scores[100:])).T\n",
    "    err = to_df(err, columns=['time', 'nees'])\n",
    "    fig = ex.line(err, x='time', y='nees')\n",
    "    \n",
    "    ci = sp.stats.chi2.ppf([0.025, 0.975], nees.dim)\n",
    "    fig.add_hline(y=ci[0], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "    fig.add_hline(y=ci[1], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ba1d3-1ca4-49a7-be6d-fa40f259e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_columns(_plot_nees_one(cv_q0_one), _plot_nees_one(cv_q1_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fef5f5-4ea7-4649-964c-691f44f0a2c9",
   "metadata": {},
   "source": [
    "Using non-zero process noise (right-hand-side above) leads to NEES scores which do not show the tendency to accumulate over time. They also seem to remain within the 95% confidence interval. This comes at the cost of statistical consistency of the filter: the mean of multiple independent runs of the filter (with different random seeds governing the measurement noise) falls within the predicted 95% confidence interval for $Q=1$ but falls well below it for $Q=1$.\n",
    "\n",
    "Using zero proces (left-hand-side above) leads to covariance estimate converging to zero which exacerbates the non-scaled error and takes it from $0.4$ (meters) to almost 20 (standard deviations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1341f48-649d-40c7-947a-d85e87d48449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drift_many(Q, target, m=100):\n",
    "    motion = ConstantVelocityModel(Q)\n",
    "    kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                     [0, 1, 0, 0, 0, 0],\n",
    "                                     [0, 0, 1, 0, 0, 0]])\n",
    "    sensor = GeometricSensor()\n",
    "    r = Runner(target, sensor, kf)\n",
    "    r.run_many(m, 500)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cd8f1-df77-463a-b0bd-ab061d91eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cache cv_q0_many = _drift_many(0, target)\n",
    "%cache cv_q1_many = _drift_many(1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3707f6d-12b4-442b-8c64-39b3b10f6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_nees_many(runner, skip=100):\n",
    "    nees = evaluate_nees(runner.many_x_hat[:, :, :3, :], runner.many_P_hat[:, :, :3, :3], runner.truth[1:, :3])\n",
    "    err = np.asarray((np.arange(runner.n-skip), nees.scores[:,skip:].mean(axis=0))).T\n",
    "    err = to_df(err, columns=['time', 'nees'])\n",
    "    fig = ex.line(err, x='time', y='nees')\n",
    "    \n",
    "    ci = sp.stats.chi2.ppf([0.025, 0.975], runner.m * nees.dim) / runner.m\n",
    "    fig.add_hline(y=ci[0], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "    fig.add_hline(y=ci[1], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f428ee-9c04-4589-a23d-f1258b255262",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_columns(_plot_nees_many(cv_q0_many), _plot_nees_many(cv_q1_many))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515b951-5359-47f2-a40b-86b0e09d1181",
   "metadata": {},
   "source": [
    "Let's compare that with non-scaled error values. Above we have mean NEES across time from 100 independent runs. With $Q=0$ (on the left) we observe statistical consistency - the mean NEES stays within the 95% confidence interval about 95% of the time. In comparison, with $Q=1$ (on the right) that mean NEES stays below the 95% CI 100% of the time. Thus, the filter is no longer statistically consistent but the covariance estimate better represents the actual error of the position estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9dd314-5899-4fc7-8a3d-4d0f8d1deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_error_many(runner):\n",
    "    tm  = np.arange(runner.n-100)\n",
    "    err = np.linalg.norm(runner.many_x_hat[:,100:,:3,0] - runner.truth[101:,:3], axis=2)\n",
    "    \n",
    "    avg = np.array((tm, err.mean(axis=0))).T\n",
    "    avg = to_df(avg, columns=['time', 'err'])\n",
    "    avg['type'] = 'avg'\n",
    "    \n",
    "    low = np.array((tm, np.quantile(err, .025, axis=0))).T\n",
    "    low = to_df(low, columns=['time', 'err'])\n",
    "    low['type'] = '.025'\n",
    "    \n",
    "    upp = np.array((tm, np.quantile(err, .975, axis=0))).T\n",
    "    upp = to_df(upp, columns=['time', 'err'])\n",
    "    upp['type'] = '.975'\n",
    "\n",
    "    err = pd.concat((avg, low, upp), axis=0).melt(['time', 'type'], ['err'], 'dim', 'error')\n",
    "    return ex.line(err, x='time', y='error', color='type', facet_row='dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91692cb-b872-4e05-8038-e7e15cfaa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_columns(_plot_error_many(cv_q0_many), _plot_error_many(cv_q1_many))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdbbff-c0e7-4963-af35-0a64a95b7081",
   "metadata": {},
   "source": [
    "As we see above, for $Q=0$:\n",
    "* `+` the average NEES across multiple runs is consistent with the predicted 95% confidence interval\n",
    "* `+` the non-scaled error is low\n",
    "* `-` NEES on individual runs can accumulate to very large values - above 15 - which means that a tracker might report fairly correct position but with extremely incorrect covariance\n",
    "\n",
    "Coversely, for $Q=1$:\n",
    "* `-` the average NEES across multiple runs is inconsistent with the predicted 95% CI\n",
    "* `-` the non-scaled error is high\n",
    "* `+` the error reported for individual runs matches the actual error much better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a271f3-311a-44b4-9a8f-e6549b5ef14a",
   "metadata": {},
   "source": [
    "As the last thing, let's take a look at the scaled (NEES) and non-scaled (linear) error in the function of process noise intensity $Q$. For each value of $Q$ we perform 100 independent runs of the Kalman Filter, each taking 500 iterations. We then calcuate the mean and the 0.975 quantile of NEES and non-scaled errors for each iteration, across all 100 independent runs. Below, you can see boxplots of those four metrics: each boxplot aggregates 400 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123527b9-cbfd-401d-844b-9cd9c341d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_metrics_1(target):\n",
    "    data = []\n",
    "    for Q in [0, .001, .002, .005, .01, .02, .05, .1, .2, .5, 1, 1.5, 2, 2.5, 5, 10, 20]:\n",
    "        run  = _drift_many(Q, target)\n",
    "        nees = evaluate_nees(run.many_x_hat[:, 100:, :3, :], run.many_P_hat[:, 100:, :3, :3], run.truth[101:, :3])\n",
    "        err  = np.linalg.norm(run.many_x_hat[:,100:,:3,0] - run.truth[101:,:3], axis=2)\n",
    "        \n",
    "        mean_nees = nees.scores.mean(axis=0)\n",
    "        q975_nees = np.quantile(nees.scores, .975, axis=0)\n",
    "    \n",
    "        mean_err = err.mean(axis=0)\n",
    "        q975_err = np.quantile(err, .975, axis=0)\n",
    "    \n",
    "        part = np.asarray((mean_nees, q975_nees, mean_err, q975_err)).T\n",
    "        part = to_df(part, columns=['nees_mean', 'nees_q975', 'err_mean', 'err_q975'])\n",
    "        part['Q'] = str(Q)\n",
    "        \n",
    "        data.append(part)\n",
    "    return pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d1bec-1efb-4dae-96cc-ac52918a30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cache cv_metrics_1 = _generate_metrics_1(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a7218-adc5-491f-823e-cb7e5c76f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, dim = 100, 3\n",
    "\n",
    "fig = ex.box(cv_metrics_1.melt(['Q'], ['nees_mean', 'nees_q975', 'err_mean', 'err_q975'], 'metric', 'value'),\n",
    "             x='Q', y='value', color='metric')\n",
    "\n",
    "ci = sp.stats.chi2.ppf([0.025, 0.975], m * dim) / m\n",
    "fig.add_hline(y=ci[0], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.add_hline(y=ci[1], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90f4ce-a6a2-4520-914e-0be31d4e092a",
   "metadata": {},
   "source": [
    "As expected, only for $Q=0$, the mean NEES falls within the 95% confidence interval. However, it is also the value of $Q$ for which the 0.975 quantile of NEES is the highest, which is due to the accumulation of error within each independent run of the filter. This is also where the non-scaled error is the lowest, which means that in any given iterations of any individual run, we expect a very precision position estimate and a very imprecise covariance estimate.\n",
    "\n",
    "The 0.975 quantile of NEES falls within the 95% confidence interval for somewhere around $Q=2$. This is where, most of the time within a single run, we can trust that the actual error of the position estimate matches the reported esimate of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead08459-aa67-42aa-8cd0-cb180c4322d5",
   "metadata": {},
   "source": [
    "Let's now take one last look at boxplots of all of the NEES and error data: each individual boxplot will aggregate $100 \\times 400$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed654d-b5c9-4e16-bede-574be59b6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_metrics_2(target):\n",
    "    data = []\n",
    "    for Q in [0, .001, .002, .005, .01, .02, .05, .1, .2, .5, 1, 1.5, 2, 2.5, 5, 10, 20]:\n",
    "        run  = _drift_many(Q, target)\n",
    "        nees = evaluate_nees(run.many_x_hat[:, 100:, :3, :], run.many_P_hat[:, 100:, :3, :3], run.truth[101:, :3])\n",
    "        err  = np.linalg.norm(run.many_x_hat[:,100:,:3,0] - run.truth[101:,:3], axis=2)\n",
    "        \n",
    "        part = np.asarray((nees.scores.reshape(-1), err.reshape(-1))).T\n",
    "        part = to_df(part, columns=['nees', 'err'])\n",
    "        part['Q'] = str(Q)\n",
    "        \n",
    "        data.append(part)\n",
    "    return pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c8d80-a842-44cb-99da-e8865514b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cache cv_metrics_2 = _generate_metrics_2(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94918362-be46-4a6e-8847-0c7d582f20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, dim = 100, 3\n",
    "\n",
    "fig = ex.box(cv_metrics_2.melt(['Q'], ['nees', 'err'], 'metric', 'value'), x='Q', y='value', color='metric')\n",
    "fig.update_traces(boxpoints=False)\n",
    "\n",
    "ci = sp.stats.chi2.ppf([0.025, 0.975], m * dim) / m\n",
    "fig.add_hline(y=ci[0], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.add_hline(y=ci[1], line_width=.5, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde037e1-ef03-42b8-819e-bd9f91aec811",
   "metadata": {},
   "source": [
    "# Multiple independent runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b53db-57cb-4270-8c93-3d398d9a688a",
   "metadata": {},
   "source": [
    "## NEES statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed0ad6-f93a-4f23-972f-f2da8f4503ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motion = ConstantVelocityModel(0)\n",
    "kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0, 0, 0],\n",
    "                                 [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "cv0 = run_many(100, 400, target, GeometricSensor(), kf)\n",
    "cv0_eval = evaluate_many(*cv0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2e9e7-e8bc-455c-aecd-0c2380a5de0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_nees(cv0_eval.position_nees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f54dd-1909-4fa6-8b50-daf0aa69cf1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motion = ConstantVelocityModel(1)\n",
    "kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0, 0, 0],\n",
    "                                 [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "cv1 = run_many(100, 400, target, GeometricSensor(), kf)\n",
    "cv1_eval = evaluate_many(*cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2c62b-b843-4f71-b8f4-98ec60c085c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_nees(cv1_eval.position_nees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339807a-fb65-4b08-b2b2-8978e9978e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motion = ConstantVelocityModel(3)\n",
    "kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0, 0, 0],\n",
    "                                 [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "cv3 = run_many(100, 400, target, GeometricSensor(), kf)\n",
    "cv3_eval = evaluate_many(*cv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c15cea-c718-4d1a-a8e9-5b7c102f746a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_nees(cv3_eval.position_nees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1376f-ed12-4fe9-b71e-31f191f08665",
   "metadata": {},
   "source": [
    "## All individual runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffdc6d-715b-4479-af1b-b4bc8bad257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(cv0_eval.position_nees.scores.shape[0]):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(375), y=cv0_eval.position_nees.scores[i,25:], mode='lines', legendgroup=i))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07961a-e609-4745-bc1e-f379d5efbaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(cv1_eval.position_nees.scores.shape[0]):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(375), y=cv1_eval.position_nees.scores[i,25:], mode='lines', legendgroup=i))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176b677-27be-436f-9092-b3860ab66013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(cv3_eval.position_nees.scores.shape[0]):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(375), y=cv3_eval.position_nees.scores[i,25:], mode='lines', legendgroup=i))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfae215-e699-4fb4-b99c-8b8393584e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = ConstantVelocityModel(0)\n",
    "kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0, 0, 0],\n",
    "                                 [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "sensor = GeometricSensor(seed=8)\n",
    "r = Runner(target, sensor, kf)\n",
    "r.run_many(1, 10000)\n",
    "\n",
    "s = evaluate_runner(r)\n",
    "s.position_nees.scores = s.position_nees.scores.reshape((1, -1))\n",
    "\n",
    "plot_nees(s.position_nees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237314b3-a820-4536-82a7-96bccb5725e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.chi2.ppf([0.025, 0.975], 10000 * 3) / 10000, s.position_nees.scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71285361-bfe8-48af-a619-6e9eb3edc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = ConstantVelocityModel(0)\n",
    "kf = LinearKalmanFilter(motion, [[1, 0, 0, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0, 0, 0],\n",
    "                                 [0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "sensor = GeometricSensor(seed=8)\n",
    "r = Runner(target, sensor, kf)\n",
    "r.run_many(100, 500)\n",
    "\n",
    "s = evaluate_runner(r)\n",
    "plot_nees(s.position_nees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039835a-0cdf-4e03-b218-e5d37732edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = sp.stats.chi2.ppf([0.025, 0.975], 500 * 3) / 500\n",
    "ci, s.position_nees.scores.mean(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53baacc3-1e5c-47bc-88a9-9d3c6b848195",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ex.histogram(x=s.position_nees.scores.mean(axis=1))\n",
    "fig.add_vline(x=ci[0])\n",
    "fig.add_vline(x=ci[1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6767d2d-6553-4874-90c8-71fa95086036",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "\n",
    "You've highlighted a critical aspect of how Kalman filters operate, particularly when process noise is small or zero. Let's delve deeper into why this is the case:\n",
    "\n",
    "#### Dependence of State Estimates Over Time:\n",
    "\n",
    "1. Recursive Nature of Kalman Filter:\n",
    "  * The Kalman filter is inherently recursive. Each state estimate depends on the previous estimate through the state transition model. This means:\n",
    "    * Prediction Step: The state prediction at time $t$ is based on the previous state estimate through the state transition model (often denoted as A for linear systems).\n",
    "    * Update Step: The final estimate incorporates the measurement at time $t$, but it's still influenced by the prediction made from the previous state, linking it back through time.\n",
    "2. Process Noise and Error Accumulation:\n",
    "  * With small-to-zero process noise, the filter assumes that the state evolves predictably according to the model without much random fluctuation. \n",
    "  * Error Accumulation: Any small errors or biases in the initial state or in the model itself will propagate through time. Without significant process noise to \"reset\" or randomize the estimates, these errors accumulate, leading to correlated errors over time rather than independent ones.\n",
    "3. NEES (Normalized Estimation Error Squared):\n",
    "  * NEES Growing: In a single run, if the process noise is very small, the NEES can grow because the filter trusts the model too much, leading to overconfidence in its predictions. As errors accumulate, the actual state deviates from what the filter predicts, and the error (squared) increases.\n",
    "  * Mean NEES and Confidence Intervals: \n",
    "    * In a single run, the mean NEES might exceed the expected statistical bounds because the estimation errors are not independent. \n",
    "    * Over many runs, the mean NEES would be consistent with the confidence interval because, across multiple independent trials, the errors can be seen as independent realizations, balancing out the accumulation effect seen in individual runs.\n",
    "\n",
    "#### Understanding the Phenomena:\n",
    "\n",
    "* Temporal Correlation: The estimates are not independent because each new estimate is influenced by all previous estimates through the dynamics model. This temporal correlation means that an error at one time step can affect subsequent steps.\n",
    "* Model Mismatch: If the true system dynamics are not perfectly captured by the model, particularly when process noise is minimal, the filter can become overly confident in its predictions, leading to a divergence from the true state.\n",
    "* Observability and Controllability: Issues with these can also lead to dependencies over time. If parts of the state are not well observed or controlled, errors can persist or grow.\n",
    "\n",
    "#### Strategies to Address or Understand This:\n",
    "\n",
    "* Increase Process Noise: Introducing more process noise can help mitigate this by allowing the filter to be less certain about the state evolution, making it more adaptive to unexpected changes or model errors.\n",
    "* Model Refinement: Better modeling of system dynamics can reduce the accumulation of errors. \n",
    "* Monte Carlo Simulations: Running multiple simulations can show how the NEES behaves across different initial conditions or noise realizations, giving a better picture of the filter's performance.\n",
    "* Analytical Tools: Use of Lyapunov stability analysis or examining the eigenvalues of the system matrix can provide insights into how errors might grow or decay over time.\n",
    "* Tuning: Carefully tuning $Q$ (process noise covariance) and $R$ (measurement noise covariance) based on real data or system understanding can help balance the filter's reliance on model predictions versus measurements.\n",
    "\n",
    "Understanding this time-dependent nature of Kalman filter estimates is crucial for applications where long-term prediction accuracy is important, especially in scenarios with low process noise or when the system model is not perfectly known."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
